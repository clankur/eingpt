{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"input.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1115394\n"
     ]
    }
   ],
   "source": [
    "print(len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You\n"
     ]
    }
   ],
   "source": [
    "print(text[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz \n",
      " 65\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "\n",
    "print(''.join(chars), \"\\n\", vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[46, 47, 1, 58, 46, 43, 56, 43]\n",
      "hi there\n"
     ]
    }
   ],
   "source": [
    "stoi = {ch: i for i, ch in enumerate(chars)}\n",
    "itos = {i:ch for i, ch in enumerate(chars)}\n",
    "encode = lambda s: [stoi[c] for c in s]\n",
    "decode = lambda l: ''.join([itos[i] for i in l])\n",
    "\n",
    "sample_str = \"hi there\"\n",
    "print(encode(sample_str))\n",
    "print(decode(encode(sample_str)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenisers\n",
    "- Example above is a character level tokeniser \n",
    "- Sentence Piece = encodes texts to integers, subword tokenizer. Unit level tokenizer, not character\n",
    "- Tiktoken = ChatGPT uses this one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = int(0.9 * len(data))\n",
    "train_data, val_data = data[:n], data[n:]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chunking data\n",
    "Transformed only works with chunks of the dataset. During training, data is sampled and sent in chunks into transformer, with a maximum blocksize. A chunk has multiple examples imbedded in it, since each examples has characters that follow it, which are used as a target for each of your predictions. For example, in a blocksize of 8, we will have 9 examples to predict the next character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_size = 8 \n",
    "train_data[:block_size+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when input is tensor([18]), the target is [tensor(47)]\n",
      "when input is tensor([18, 47]), the target is [tensor(56)]\n",
      "when input is tensor([18, 47, 56]), the target is [tensor(57)]\n",
      "when input is tensor([18, 47, 56, 57]), the target is [tensor(58)]\n",
      "when input is tensor([18, 47, 56, 57, 58]), the target is [tensor(1)]\n",
      "when input is tensor([18, 47, 56, 57, 58,  1]), the target is [tensor(15)]\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15]), the target is [tensor(47)]\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47]), the target is [tensor(58)]\n"
     ]
    }
   ],
   "source": [
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size+1]\n",
    "\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(f\"when input is {context}, the target is {[target]}\") "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looked at time dimension (a sequence, where time refers to the index of the sequence in this case think of it as the characters) but have to look at batch dimension\n",
    "The batch size is the # of independent sequences we've processed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs\n",
      "torch.Size([4, 8])\n",
      "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
      "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
      "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
      "        [25, 17, 27, 10,  0, 21,  1, 54]])\n",
      "targets\n",
      "torch.Size([4, 8])\n",
      "tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
      "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
      "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
      "        [17, 27, 10,  0, 21,  1, 54, 39]])\n",
      "---\n",
      "when input is [24], the target is [tensor(43)]\n",
      "when input is [24, 43], the target is [tensor(58)]\n",
      "when input is [24, 43, 58], the target is [tensor(5)]\n",
      "when input is [24, 43, 58, 5], the target is [tensor(57)]\n",
      "when input is [24, 43, 58, 5, 57], the target is [tensor(1)]\n",
      "when input is [24, 43, 58, 5, 57, 1], the target is [tensor(46)]\n",
      "when input is [24, 43, 58, 5, 57, 1, 46], the target is [tensor(43)]\n",
      "when input is [24, 43, 58, 5, 57, 1, 46, 43], the target is [tensor(39)]\n",
      "when input is [44], the target is [tensor(53)]\n",
      "when input is [44, 53], the target is [tensor(56)]\n",
      "when input is [44, 53, 56], the target is [tensor(1)]\n",
      "when input is [44, 53, 56, 1], the target is [tensor(58)]\n",
      "when input is [44, 53, 56, 1, 58], the target is [tensor(46)]\n",
      "when input is [44, 53, 56, 1, 58, 46], the target is [tensor(39)]\n",
      "when input is [44, 53, 56, 1, 58, 46, 39], the target is [tensor(58)]\n",
      "when input is [44, 53, 56, 1, 58, 46, 39, 58], the target is [tensor(1)]\n",
      "when input is [52], the target is [tensor(58)]\n",
      "when input is [52, 58], the target is [tensor(1)]\n",
      "when input is [52, 58, 1], the target is [tensor(58)]\n",
      "when input is [52, 58, 1, 58], the target is [tensor(46)]\n",
      "when input is [52, 58, 1, 58, 46], the target is [tensor(39)]\n",
      "when input is [52, 58, 1, 58, 46, 39], the target is [tensor(58)]\n",
      "when input is [52, 58, 1, 58, 46, 39, 58], the target is [tensor(1)]\n",
      "when input is [52, 58, 1, 58, 46, 39, 58, 1], the target is [tensor(46)]\n",
      "when input is [25], the target is [tensor(17)]\n",
      "when input is [25, 17], the target is [tensor(27)]\n",
      "when input is [25, 17, 27], the target is [tensor(10)]\n",
      "when input is [25, 17, 27, 10], the target is [tensor(0)]\n",
      "when input is [25, 17, 27, 10, 0], the target is [tensor(21)]\n",
      "when input is [25, 17, 27, 10, 0, 21], the target is [tensor(1)]\n",
      "when input is [25, 17, 27, 10, 0, 21, 1], the target is [tensor(54)]\n",
      "when input is [25, 17, 27, 10, 0, 21, 1, 54], the target is [tensor(39)]\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "batch_size = 4 # how many independent sequences to process at once in parallel\n",
    "block_size = 8 # what is the maximum context length for predictions\n",
    "\n",
    "def get_batch(split: str):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, size=(batch_size,)) # will return batch_size random numbers that are offsets of the data set \n",
    "    x = torch.stack([data[i:i+block_size] for i in ix]) # builds a stack of tensors of size blocksize for each random number in ix\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix]) # offset by 1 stack of tensors\n",
    "    return x, y\n",
    "xb, yb = get_batch('train')\n",
    "print(\"inputs\")\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "\n",
    "print(\"targets\") # help for loss function\n",
    "print(yb.shape)\n",
    "print(yb)\n",
    "\n",
    "print(\"---\")\n",
    "\n",
    "for b in range(batch_size): # batch dimension\n",
    "    for t in range(block_size): # time dimension\n",
    "        context = xb[b, :t+1]\n",
    "        target = yb[b,t]\n",
    "        print(f\"when input is {context.tolist()}, the target is {[target]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
      "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
      "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
      "        [25, 17, 27, 10,  0, 21,  1, 54]])\n"
     ]
    }
   ],
   "source": [
    "print(xb) # our input to the transformer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion\n",
    "So far we just implented how to get our input batches for our LLM. And the y represents our desired targets which are used in the loss function. Now we need to plug it into a language model. The simples on we can use is the bigram language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 65])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from typing import List, Tuple\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size: int) -> None:   \n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "    \n",
    "    def forward(self, idx: torch.Tensor, targets:torch.Tensor = None) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        performs a forward pass of the model\n",
    "\n",
    "        Parameters:\n",
    "        - idx: a [B, T] tensor of integers representing the input sequence\n",
    "        - targets: a [B, T] tensor of integers representing the output sequence\n",
    "\n",
    "        Returns:\n",
    "        - logits: a [B*T, C] tensor of non-normalized scores over the vocabulary\n",
    "        - loss: a scalar loss value if targets is not None\n",
    "        \"\"\"\n",
    "        logits = self.token_embedding_table(idx)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else: \n",
    "            # reshape logits and targets to [B*T, C] and [B*T] respectively\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        return logits, loss        \n",
    "    \n",
    "    def generate (self, idx: torch.Tensor, max_new_tokens:int) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        generates the next `max_token_len` tokens given an input sequence \n",
    "\n",
    "        Parameters:\n",
    "        - idx: a [B, T] tensor of integers representing the input sequence\n",
    "        - max_token_len: the maximum number of tokens to generate\n",
    "        \"\"\"\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get the predictions for the next token\n",
    "            logits, loss = self.forward(idx)\n",
    "            # focus on the last token\n",
    "            logits = logits[:, -1, :] # this becomes [B, C]\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # a [B, C] tensor\n",
    "            # sample and get the next token\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # this is a [B, 1] tensor\n",
    "            idx = torch.cat([idx, idx_next], dim=-1) # becomes [B, T+1]\n",
    "        return idx\n",
    "            \n",
    "\n",
    "m = BigramLanguageModel(vocab_size)\n",
    "out, loss = m(xb, yb)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a pytorch optimizer\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.536496639251709\n"
     ]
    }
   ],
   "source": [
    "# training the model\n",
    "batch_size = 64\n",
    "for steps in range(1000):\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = m(xb, yb) \n",
    "    optimizer.zero_grad(set_to_none=True) # clear the gradients\n",
    "    loss.backward() # compute gradients\n",
    "    optimizer.step() # update parameters\n",
    "\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "O:\n",
      "MARToth ar strofourres awe in t we N thnghel bo we asbld agenawsetouthengr oore,\n",
      "Pamy sean.\n",
      "AR:\n",
      "Sw:\n",
      "JUCoun fed; or y: ileme--ceves ieeste wind teld anomen po;\n",
      "H: f hedmize ckndoth youckn ave CE:\n",
      "CINoplld?\n",
      "S:\n",
      "I'thavisieasse?\n",
      "Fofong yom heancknotherl my twiey, shoue.\n",
      "ENRYothem bel itllin ulindit. blld we wicr, pasurean gagead, at thistwerslat, wsolau iato\n",
      "'sh.\n",
      "SI ins\n",
      "Whan be by.\n",
      "\n",
      "ENUSThe anourincth LEiller feng:\n",
      "\n",
      "Hannewerely foutoinrevit spous.\n",
      "Rea inou, r bedoou Inoraindongid ay ayowim:\n",
      "Berday weavevemy s.\n",
      "Hansots, hoacomowollupr m\n",
      "BEd ru t hale u ierve,\n",
      "Fr ay ino bremale l oth th de honorithanor f t bupong, Thehy ngagld cth hased,\n",
      "\n",
      "\n",
      "BEMugor coreshat wio thegouris whome pllad,\n",
      "\n",
      "\n",
      "\n",
      "SOK:\n",
      "Myedan m mathe w\n",
      "LEr:\n",
      "TING I y wouray thet mane ambo\n",
      "Why:\n",
      "\n",
      "stheryofit\n",
      "NS w how mso t f ghen.\n",
      "LULICOfrd t y st o:\n",
      "Pe? tille RENI m hat IOPOKINLOLLKAnier the,\n",
      "\n",
      "HAndstittrere hem:\n",
      "Hot me be iceis tofonge mpot?\n",
      "WIOLOfe bur te.\n",
      "ADI ithorlleupo thinttth!\n",
      "With nclos ses RDr'd, aur d t I nd\n",
      "Be\n",
      "ARUK:\n",
      "F r t, id\n"
     ]
    }
   ],
   "source": [
    "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=1000)[0].tolist()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "54237b2ada78b38c76b6097948e962d6c612f44590e99727b54d6fd40c119181"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
