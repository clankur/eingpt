{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"input.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1115394\n"
     ]
    }
   ],
   "source": [
    "print(len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You\n"
     ]
    }
   ],
   "source": [
    "print(text[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz \n",
      " 65\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "\n",
    "print(''.join(chars), \"\\n\", vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[46, 47, 1, 58, 46, 43, 56, 43]\n",
      "hi there\n"
     ]
    }
   ],
   "source": [
    "stoi = {ch: i for i, ch in enumerate(chars)}\n",
    "itos = {i:ch for i, ch in enumerate(chars)}\n",
    "encode = lambda s: [stoi[c] for c in s]\n",
    "decode = lambda l: ''.join([itos[i] for i in l])\n",
    "\n",
    "sample_str = \"hi there\"\n",
    "print(encode(sample_str))\n",
    "print(decode(encode(sample_str)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenisers\n",
    "- Example above is a character level tokeniser \n",
    "- Sentence Piece = encodes texts to integers, subword tokenizer. Unit level tokenizer, not character\n",
    "- Tiktoken = ChatGPT uses this one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = int(0.9 * len(data))\n",
    "train_data, val_data = data[:n], data[n:]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chunking data\n",
    "Transformed only works with chunks of the dataset. During training, data is sampled and sent in chunks into transformer, with a maximum blocksize. A chunk has multiple examples imbedded in it, since each examples has characters that follow it, which are used as a target for each of your predictions. For example, in a blocksize of 8, we will have 9 examples to predict the next character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_size = 8 \n",
    "train_data[:block_size+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when input is tensor([18]), the target is [tensor(47)]\n",
      "when input is tensor([18, 47]), the target is [tensor(56)]\n",
      "when input is tensor([18, 47, 56]), the target is [tensor(57)]\n",
      "when input is tensor([18, 47, 56, 57]), the target is [tensor(58)]\n",
      "when input is tensor([18, 47, 56, 57, 58]), the target is [tensor(1)]\n",
      "when input is tensor([18, 47, 56, 57, 58,  1]), the target is [tensor(15)]\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15]), the target is [tensor(47)]\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47]), the target is [tensor(58)]\n"
     ]
    }
   ],
   "source": [
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size+1]\n",
    "\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(f\"when input is {context}, the target is {[target]}\") "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looked at time dimension (a sequence, where time refers to the index of the sequence in this case think of it as the characters) but have to look at batch dimension\n",
    "The batch size is the # of independent sequences we've processed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs\n",
      "torch.Size([4, 8])\n",
      "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
      "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
      "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
      "        [25, 17, 27, 10,  0, 21,  1, 54]])\n",
      "targets\n",
      "torch.Size([4, 8])\n",
      "tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
      "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
      "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
      "        [17, 27, 10,  0, 21,  1, 54, 39]])\n",
      "---\n",
      "when input is [24], the target is [tensor(43)]\n",
      "when input is [24, 43], the target is [tensor(58)]\n",
      "when input is [24, 43, 58], the target is [tensor(5)]\n",
      "when input is [24, 43, 58, 5], the target is [tensor(57)]\n",
      "when input is [24, 43, 58, 5, 57], the target is [tensor(1)]\n",
      "when input is [24, 43, 58, 5, 57, 1], the target is [tensor(46)]\n",
      "when input is [24, 43, 58, 5, 57, 1, 46], the target is [tensor(43)]\n",
      "when input is [24, 43, 58, 5, 57, 1, 46, 43], the target is [tensor(39)]\n",
      "when input is [44], the target is [tensor(53)]\n",
      "when input is [44, 53], the target is [tensor(56)]\n",
      "when input is [44, 53, 56], the target is [tensor(1)]\n",
      "when input is [44, 53, 56, 1], the target is [tensor(58)]\n",
      "when input is [44, 53, 56, 1, 58], the target is [tensor(46)]\n",
      "when input is [44, 53, 56, 1, 58, 46], the target is [tensor(39)]\n",
      "when input is [44, 53, 56, 1, 58, 46, 39], the target is [tensor(58)]\n",
      "when input is [44, 53, 56, 1, 58, 46, 39, 58], the target is [tensor(1)]\n",
      "when input is [52], the target is [tensor(58)]\n",
      "when input is [52, 58], the target is [tensor(1)]\n",
      "when input is [52, 58, 1], the target is [tensor(58)]\n",
      "when input is [52, 58, 1, 58], the target is [tensor(46)]\n",
      "when input is [52, 58, 1, 58, 46], the target is [tensor(39)]\n",
      "when input is [52, 58, 1, 58, 46, 39], the target is [tensor(58)]\n",
      "when input is [52, 58, 1, 58, 46, 39, 58], the target is [tensor(1)]\n",
      "when input is [52, 58, 1, 58, 46, 39, 58, 1], the target is [tensor(46)]\n",
      "when input is [25], the target is [tensor(17)]\n",
      "when input is [25, 17], the target is [tensor(27)]\n",
      "when input is [25, 17, 27], the target is [tensor(10)]\n",
      "when input is [25, 17, 27, 10], the target is [tensor(0)]\n",
      "when input is [25, 17, 27, 10, 0], the target is [tensor(21)]\n",
      "when input is [25, 17, 27, 10, 0, 21], the target is [tensor(1)]\n",
      "when input is [25, 17, 27, 10, 0, 21, 1], the target is [tensor(54)]\n",
      "when input is [25, 17, 27, 10, 0, 21, 1, 54], the target is [tensor(39)]\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "batch_size = 4 # how many independent sequences to process at once in parallel\n",
    "block_size = 8 # what is the maximum context length for predictions\n",
    "\n",
    "def get_batch(split: str):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, size=(batch_size,)) # will return batch_size random numbers that are offsets of the data set \n",
    "    x = torch.stack([data[i:i+block_size] for i in ix]) # builds a stack of tensors of size blocksize for each random number in ix\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix]) # offset by 1 stack of tensors\n",
    "    return x, y\n",
    "xb, yb = get_batch('train')\n",
    "print(\"inputs\")\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "\n",
    "print(\"targets\") # help for loss function\n",
    "print(yb.shape)\n",
    "print(yb)\n",
    "\n",
    "print(\"---\")\n",
    "\n",
    "for b in range(batch_size): # batch dimension\n",
    "    for t in range(block_size): # time dimension\n",
    "        context = xb[b, :t+1]\n",
    "        target = yb[b,t]\n",
    "        print(f\"when input is {context.tolist()}, the target is {[target]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
      "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
      "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
      "        [25, 17, 27, 10,  0, 21,  1, 54]])\n"
     ]
    }
   ],
   "source": [
    "print(xb) # our input to the transformer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion\n",
    "So far we just implented how to get our input batches for our LLM. And the y represents our desired targets which are used in the loss function. Now we need to plug it into a language model. The simples on we can use is the bigram language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x249aaec2070>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size: int) -> None:   \n",
    "        super().__init__()\n",
    "        self.toke_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "    \n",
    "    def forward(self, idx, targets):\n",
    "m = BigramLanguageModel(vocab_size)\n",
    "out = m(xb, yb)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "54237b2ada78b38c76b6097948e962d6c612f44590e99727b54d6fd40c119181"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
